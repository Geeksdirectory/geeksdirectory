
### Syllabus
![[Pasted image 20241106140819.png]]

### Big data and its characteristics

- **Big Data** refers to extremely ==large and complex datasets== that traditional data processing software cannot handle effectively.
- It ==involves the collection, processing, and analysis of large volumes== of diverse and ==rapidly generated data==.

#### Characteristics of Big Data (The 5 Vs)

![[Pasted image 20241106141732.png]]

Big Data is generally defined by five main characteristics, often referred to as the **5 Vs**:

- **Volume**:
    
    - Refers to the _amount of data_ generated, which is massive in scale.
    - Examples include data from social media, online transactions, sensors, and mobile devices.
    - Requires large storage and distributed systems for processing.
- **Velocity**:
    
    - Describes the ==speed== at which data is ==generated, processed, and analyzed==
    - Real-time or near-real-time data processing is crucial for timely decision-making.
    - Examples include social media updates, stock trading, and streaming data.

- **Variety**:
    - Indicates the different types and ==formats== of data, both ==structured and unstructured.==
    - Examples include text, images, videos, audio, and social media interactions.
    - Requires techniques to handle unstructured data and extract insights from various formats.

- **Veracity**:
    
    - Refers to the ==_accuracy, quality, and reliability_ of data==.
    - ==Data can be incomplete, inconsistent, or inaccurate, which impacts analysis.==
    - Handling veracity requires data cleaning and validation techniques to ensure trustworthy insights.

- **Value**:
    
    - The most important V, representing the ==_usefulness and insights_== that can be derived from Big Data.
    - Turning data into actionable insights or ==added value for businesses==, such as improving decision-making or enhancing customer experiences.


### Types of Big data 

- **Structured Data**:
    - ==Organized in a predefined format==, typically in ==rows and columns== (like relational databases).
    - Easy to store, process, and analyze using traditional tools such as SQL.
    - Examples: ==Financial transactions, inventory data, customer data, and sensor readings.==

- **Unstructured Data**:
    - Has no ==predefined structure== and doesn’t fit neatly into rows and columns.
    - ==More complex to process and analyze as it requires specialized tools for handling.==
    - Examples: Social media ==posts, videos, images, emails, audio recordings, and web pages.==

- **Semi-Structured Data**:
    - Contains elements of== both structured and unstructured data.==
    - Has an overall structure but lacks strict formatting (often in tagged formats).
    - Examples: ==JSON and XML files==, emails with metadata, and log files.

####  Types of Big Data Based on Sources

- **Human-Generated Data**:
    - Data generated by humans through interaction with various digital platforms and devices.
    - Examples include social media posts, blog articles, emails, customer feedback, and online transaction records.

- **Machine-Generated Data**:
    - Produced by machines or devices without direct human input, often through ==sensors or automated processes==.
    - Examples include sensor data from IoT devices, server logs, GPS data, industrial machine logs, and weather sensors.

- **Process-Mediated Data**:
    - Generated as a ==byproduct of business processes or activities.==
    - Often comes from enterprise applications like ==ERP, CRM, and financial systems.==
    - Examples include supply chain data, sales and transaction records, and purchase histories.

#### Types of Big Data Based on Analytics and Use Cases

- **Operational Big Data**:
    - ==Generated from daily operations and business activities==, often in real-time.
    - Used for process optimization, real-time analytics, and decision-making.
    - Examples: Log data, transaction records, and website activity.

- **Analytical Big Data**:
    - Data gathered, processed, and analyzed to ==derive insights for strategic decision-making.==
    - Typically ==historical data used for pattern recognition==, ==trend analysis, and forecasting.==
    - Examples: Market analysis data, historical customer behavior, and product development insights


### Traditional VS Big Data

![[Pasted image 20241106144517.png]]

### Hadoop Ecosystem

![[Pasted image 20241106165309.png]]

Hadoop is an **open-source, distributed computing framework** designed for processing and storing vast amounts of **data across multiple servers**

==Developed by the Apache Software Foundation==, Hadoop allows massive datasets to be processed in parallel on large ==clusters== of ==commodity hardware==, providing fault tolerance, scalability, and flexibility.

It’s commonly used for Big Data processing and analytics due to its ability to ==handle structured, semi-structured, and unstructured data.==

#### Core Components of Hadoop
##### 1. **Hadoop Distributed File System (HDFS)**

![[Pasted image 20241106165353.png]]
![[Pasted image 20241106165405.png]]


- **Purpose**: ==HDFS is the primary storage system of Hadoop==, designed to store and manage large datasets across distributed machines.
- **Key Features**:
    - ==Distributed Storage==: Splits data into large blocks (default 128 MB) and distributes them across multiple nodes in the cluster.
    - ==Fault Tolerance==: Data blocks are ==replicated (typically 3 copies==) across nodes, so if a node fails, the data can still be accessed from another replica.
    - ==High Throughput==: Optimized for fast read and write operations, essential for large-scale batch processing.
- **Architecture**:
    - ==NameNode==: The ==master node== that manages the metadata (information about file locations and replicas) of the HDFS.
    - ==DataNode==: The ==worker nodes== that store the actual data blocks and report back to the NameNode.

##### 2. MapReduce

![[Pasted image 20241106165323.png]]

![[Pasted image 20241106165337.png]]

- Purpose: MapReduce is Hadoop’s processing model, allowing ==parallel processing of large datasets across clusters==.
- Key Features:
    - ==Parallel Processing==: Distributes tasks across multiple nodes, enabling efficient processing of massive data.
    - ==Fault Tolerance==: If a node fails, Hadoop reassigns the task to another node to complete it.

Architecture:
- Map Task: The first step in the process, where data is ==split into key-value pairs and processed in parallel across nodes.==
- Reduce Task: The second step, where ==intermediate data from Map tasks is consolidated to produce the final output.==

##### 3. YARN (Yet Another Resource Negotiator)

- Purpose: YARN is the ==resource management layer in Hadoop==, which ==allocates== and ==manages== ==resources (CPU, memory) ==across applications in the cluster.
- Key Features:
    - ==Resource Allocation==: Efficiently allocates resources based on application demands, supporting various workloads.
    - ==Multi-Tenancy==: Allows multiple applications to run concurrently, improving cluster utilization.
- Architecture:
    - ==ResourceManager==: The master node that allocates resources to various applications.
    - ==NodeManager==: Runs on each ==DataNode==, monitoring resources and reporting back to the ResourceManager.

##### 4. Hadoop Common

- Purpose: ==Provides libraries, utilities, and Java-based modules== essential for Hadoop’s core functionality.
- Key Features:
    - ==Cross-Component Support:== Supplies essential tools, such as Java libraries, to enable communication and data access across different Hadoop components.
    - ==Utilities: Supports Hadoop’s core operations and configuration==.

#### Hadoop Ecosystem

The Hadoop ecosystem consists of a variety of tools and technologies that complement the core Hadoop components to provide a more comprehensive data processing and analytics solution.

**Data Ingestion and Storage Tools**
- **Apache Scoop**: ==Used to transfer data between Hadoop and relational databases (like MySQL, Oracle==). It’s effective for batch data transfer. 
- **Apache HBase**: ==A NoSQL database that runs on top of HDFS, providing real-time read/write access to large datasets==. Ideal for semi-structured data storage.

**Data Processing and Computation Tools**
- **Apache Hive**: A ==data warehousing tool that enables SQL-like querying== (HiveQL) on Hadoop, making it accessible to users familiar with SQL.
- **Apache Pig**: A ==high-level platform for data transformation==, allowing users to write ==scripts== in Pig Latin for data processing. It’s optimized for complex data processing jobs.
- **Apache Spark**: A fast, ==general-purpose cluster computing engine== that can handle ==real-time data processing==, in-memory computing, and a wider range of workloads than MapReduce

**Data Management and Workflow Orchestration**
- **Apache Zookeeper**: A centralized service that manages distributed applications by ==maintaining configuration information==, ==naming, and synchronization.==

**Data Access and Querying Tools**
- **Apache HCatalog**: A ==metadata management service== that integrates with Hive and allows for a centralized view of data across different Hadoop tools.
![[Pasted image 20241106165045.png]]