### Syllabus
![alt text](Pastedimage20241108163545.png)

### Pagerank

![alt text](Pastedimage20241108163925.png)


==PageRank is a link analysis algorithm== developed by Larry Page and Sergey Brin, the co-founders of Google, to ==rank websites in search engine results==. It uses the ==link structure of the web to measure== the ==importance== or ==authority== of a webpage. A page that has many links to it, especially from other ==high-ranking pages==, is considered more important and thus receives a higher PageRank score.

#### Concept of PageRank

PageRank works on the principle that a page is important if many other important pages link to it. This recursive idea of importance helps rank webpages by their perceived "==authority==" in the web structure, where links are essentially treated as votes. However, links from pages that themselves are linked by many other pages carry more weight.

PageRank is based on a random surfer model, where a "==random surfer==" starts on a page and either:
1. Clicks a random link on the page (following links), or
2. Jumps to a completely random page with a certain probability.

The algorithm assigns higher ranks to pages where there is a high probability that the random surfer will end up, due to both the number and quality of links pointing to it.

#### Mathematical Formulation of PageRank

Given a web graph where pages link to each other, PageRank calculates the rank of a page \( P \) as follows:

#### PageRank Formula
The PageRank of page \( P \), denoted as \( PR(P) \), is defined by the following formula:

$$PR(P) = \frac{1 - d}{N} + d \sum_{i=1}^{k} \frac{PR(P_i)}{L(P_i)}$$


Where:
- **\( d \)**: Damping factor, usually set to 0.85. This represents the probability that the random surfer continues following links (and not jumping to a random page).
- **\( N \)**: Total number of pages on the web.
- **\( k \)**: Number of pages linking to \( P \).
- **\( P_i \)**: A page that links to \( P \).
- **\( PR(P_i) \)**: The PageRank score of page \( P_i \).
- **\( L(P_i) \)**: The number of outbound links on page \( P_i \).

The formula combines two parts:
1. **Random Jump Probability** \( \left(\frac{1 - d}{N}\right) \)**: This is the probability that the surfer jumps to a random page.
2. **Link Contribution** \( \left( d \sum_{i=1}^{k} \frac{PR(P_i)}{L(P_i)} \right) \)**: The probability that the surfer follows links from pages that link to \( P \), weighted by their PageRank and the number of links on those pages.

#### Understanding the Components

1. **Damping Factor (d)**: Controls the ==likelihood of a random jump==. Setting \( d \) to ==0.85 means there's an 85% ==chance the surfer will follow a link on a page and a 15% chance they will jump to a random page. This factor prevents "sink" pages (pages with no outgoing links) from distorting the rankings and ensures convergence of the PageRank computation.

2. **Link Contribution**: Each page that links to page \( P \) contributes a fraction of its own PageRank to \( P \). The contribution of each linking page is divided by the number of outbound links on that page, representing that each link distributes the linking page’s PageRank equally.

3. **Iteration and Convergence**: PageRank values are computed iteratively. Starting with an initial guess (often equal values for all pages), the PageRank values are updated in each iteration using the formula until they converge (i.e., the values change very little between iterations).

#### Example Calculation of PageRank

Consider a small web with 3 pages, \( A \), \( B \), and \( C \), and the following link structure:
- \( A \) links to \( B \) and \( C \).
- \( B \) links to \( C \).
- \( C \) links to \( A \).

Assume a damping factor \( d = 0.85 \).

1. **Initial Values**: Start with an initial PageRank of 1 for each page.
2. **Iterate using PageRank formula**: Apply the formula iteratively for each page until the values converge.

Each iteration might look like:
- **PR(A)**: Based on incoming links from \( C \),
- **PR(B)**: Based on incoming links from \( A \),
- **PR(C)**: Based on incoming links from \( A \) and \( B \).

After several iterations, the values converge to stable scores reflecting the "importance" of each page in this mini-network.

#### Implementation Steps

To compute PageRank for a large web graph, the following steps are typically used:

1. **Initialize**: Assign an initial PageRank to each page, typically \( \frac{1}{N} \) where \( N \) is the total number of pages.
2. **Iterate**: For each page, update its PageRank using the contributions from all linked pages.
3. **Convergence Check**: After each iteration, check if the PageRank values have converged (i.e., change by less than a predefined small threshold).
4. **Normalization**: In practice, PageRank values are often normalized so that they sum up to 1.

#### Properties of PageRank

1. **Probabilistic Interpretation**: PageRank can be interpreted as the steady-state distribution of a ==Markov chain==, where each page is a state, and links represent transition probabilities.
2. **Global Ranking**: PageRank provides a global ranking of pages, making it useful for ordering search results in search engines.
3. **Resilience to Manipulation**: Because PageRank considers the number and quality of inbound links, it is more resilient to manipulation than simpler ranking methods based solely on the number of links.

#### Applications of PageRank

1. ==**Search Engines==**: Initially used by Google to rank web pages, PageRank remains foundational in understanding the relevance of pages to search queries.
2. **==Social Network Analysis==**: Used to find influential nodes in social networks.
3. **==Recommendation Systems==**: Applied in systems that recommend items or products based on link structures or user connections.
4. **==Scientific Research==**: Used in ranking academic papers based on citation networks.
5. **==Bioinformatics==**: For identifying important genes or proteins within networks of biological interactions.

#### Advantages and Limitations of PageRank

##### Advantages
- **Quality of Results**: Considers both the quantity and quality of links, providing a more nuanced and reliable metric than raw link counts.
- **Efficient Calculation**: Can be computed on very large graphs with iterative techniques.
- **Interpretability**: Has an intuitive probabilistic interpretation that relates to user behavior on the web.

##### Limitations
- **Computational Cost**: Although efficient, computing PageRank on very large networks (like the entire web) can be resource-intensive.
- **Spam and Manipulation**: While resistant to simple link spam, sophisticated link farming and manipulative SEO techniques can still influence PageRank.
- **Static Nature**: PageRank doesn’t account for content relevance or the context of the search query itself. Modern search engines combine it with other ranking factors.

#### Modern Modifications and Variants

1. **Personalized PageRank**: A variant where the random surfer has a higher probability of jumping to pages of personal interest, often used for recommendation systems.
2. **Topic-Sensitive PageRank**: This modifies the random jump to be more likely to visit pages related to a particular topic.
3. **TrustRank**: A version of PageRank that prioritizes links from trusted sources, helping to reduce the influence of spammy or low-quality links.

#### Summary

PageRank is a powerful and influential algorithm that transformed how web pages are ranked in search engines by considering the structure of the web and the quality of links. Although it is only one component of modern ranking systems, its fundamental idea of "link authority" remains a central concept in web search, network analysis, and beyond. Its adaptability to various domains has made it one of the most impactful algorithms in data science and information retrieval.

---

### Recommendation systems

Recommendation systems are essential in modern applications to help users find items of interest based on their preferences and past interactions. There are several models for recommendation systems, with **content-based recommendation** and **collaborative filtering** being two of the most widely used approaches. Let’s dive into each of these, discussing their mechanisms, strengths, limitations, and practical examples.

#### 1. Content-Based Recommendation Systems

In **content-based recommendation systems**, the system recommends items that are similar to items the user has liked or interacted with in the past. These recommendations are based solely on the content or features of the items and the user’s previous behaviors or preferences.

#### How It Works

1. **Profile Creation**:
   - A user profile is created by analyzing items the user has previously interacted with. For example, in a movie recommendation system, if a user has watched several action movies, the system will understand that this user has a preference for the "action" genre.
   - The profile consists of features or keywords that describe the user’s preferences.

2. **Item Representation**:
   - Each item is represented by a set of attributes (e.g., for a movie, attributes might include genre, director, cast, etc.).
   - Often, **TF-IDF** (Term Frequency-Inverse Document Frequency) or **word embeddings** (like Word2Vec or BERT) are used to represent the content of textual data in numerical form.

3. **Matching**:
   - The system recommends items that are similar to the user profile by calculating the similarity between item features and the user profile.
   - **Cosine similarity** and **Euclidean distance** are common measures used to find items similar to those the user liked before.

#### Strengths of Content-Based Recommendations

- **Personalized Recommendations**: It can provide recommendations specific to individual preferences.
- **No Cold Start for Items**: New items can be recommended as long as they have content features, making it ideal for recommending newly added items.

#### Limitations of Content-Based Recommendations

- **Limited Discovery**: The system can’t recommend items outside the user’s interests, leading to a “filter bubble” effect.
- **Feature Engineering Requirement**: It requires well-defined and relevant item features, which can be challenging for complex items.

#### Example of Content-Based Recommendation

A news website might use a content-based recommendation system to suggest articles based on a user’s reading history. If a user often reads articles tagged "technology" and "startups," the system will recommend more articles with similar tags.

---

#### 2. Collaborative Filtering

**Collaborative filtering** makes recommendations based on the behavior and preferences of other users. It doesn’t rely on the content of the items but instead finds patterns in the interactions between users and items.

There are two main types of collaborative filtering:
- **User-Based Collaborative Filtering**
- **Item-Based Collaborative Filtering**

#### A. User-Based Collaborative Filtering

This approach recommends items that users with similar preferences liked in the past. 

**How It Works**:
1. **Identify Similar Users**: Find users who have similar preferences by analyzing user-item interactions (like ratings or clicks).
2. **Recommend Items**: Once similar users are identified, items that those similar users liked and that the active user hasn’t interacted with are recommended.

**Example**: If User A and User B have similar tastes, items that User B liked but User A hasn’t seen can be recommended to User A.

#### B. Item-Based Collaborative Filtering

In item-based collaborative filtering, the algorithm recommends items that are similar to items the user has already liked.

**How It Works**:
1. **Identify Similar Items**: Calculate similarities between items based on user interactions.
2. **Recommend Based on Similarity**: Recommend items similar to those the user has already interacted with.

**Example**: In an e-commerce setting, if a user bought or viewed a specific product, the system recommends similar products that other users viewed or purchased in addition to the original product.

#### Collaborative Filtering Algorithm (Matrix Factorization)

One common approach for collaborative filtering is **matrix factorization**, specifically **Singular Value Decomposition (SVD)** or **Alternating Least Squares (ALS)**. Matrix factorization techniques decompose the user-item interaction matrix into two smaller matrices, representing latent factors for users and items. This factorization captures hidden patterns and similarities that can be used for recommendation.

**Steps**:
1. **Create a User-Item Matrix**: Each cell represents a user’s rating or interaction with an item.
2. **Decompose the Matrix**: Use SVD to factorize the matrix into two matrices representing user and item latent factors.
3. **Predict Interactions**: Reconstruct the matrix by multiplying these latent matrices to predict missing interactions.

#### Strengths of Collaborative Filtering

- **Does Not Rely on Item Features**: Works purely on user interactions, making it applicable across domains.
- **Potential for Discovery**: Recommends items outside of the user’s known interests, leading to serendipitous discovery.

#### Limitations of Collaborative Filtering

- **Cold Start Problem**: It requires historical user interactions, which makes it challenging to recommend items to new users or for new items.
- **Sparsity**: Real-world datasets are often sparse, as users interact with only a small subset of items, which can make it difficult to identify patterns.

---

#### 3. Hybrid Recommendation Systems

Hybrid recommendation systems combine content-based and collaborative filtering approaches to leverage the strengths of both and address their limitations.

**Approaches**:
1. **Weighted Hybrid**: Combining the scores from content-based and collaborative models.
2. **Switching Hybrid**: Switching between content-based and collaborative filtering depending on the context (e.g., using content-based for new users).
3. **Feature-Augmented Hybrid**: Using content features to improve collaborative filtering or vice versa.

---

#### Example Models for Content-Based and Collaborative Filtering

#### Content-Based Model Example: TF-IDF with Cosine Similarity
1. Use **TF-IDF** to create a vector representation for each item (e.g., a movie or article).
2. Calculate a user profile vector by averaging the vectors of items they liked.
3. Recommend items by finding those with the highest cosine similarity to the user profile vector.

#### Collaborative Filtering Model Example: Matrix Factorization (SVD)

In a movie recommendation system:
1. Create a matrix of user ratings for movies.
2. Factorize this matrix to get latent factors for users and movies.
3. Use these factors to predict ratings for unrated movies, recommending movies with the highest predicted ratings.

---

#### Practical Example of Recommendation System

Consider Netflix:
- **Content-Based**: If a user watches several science fiction movies, the system identifies "science fiction" as a preferred genre and recommends other science fiction titles.
- **Collaborative Filtering**: Netflix also looks at what similar users have watched. If many users with similar tastes watched a particular thriller, it recommends that thriller to this user.

#### Summary

| Model                  | Description                                                                                             | Strengths                                           | Limitations                                         |
|------------------------|---------------------------------------------------------------------------------------------------------|-----------------------------------------------------|-----------------------------------------------------|
| Content-Based          | Recommends items similar to those the user has interacted with, based on item features.                | Personalized, works for new items                   | Limited discovery, requires well-defined features   |
| Collaborative Filtering | Recommends items based on the interactions of similar users or similar items, without needing content. | Discovers new interests, works across domains       | Cold start, data sparsity                           |
| Hybrid                 | Combines content-based and collaborative filtering to balance strengths and weaknesses.                 | Addresses cold start, combines personalization with discovery | Increased complexity

https://www.ques10.com/p/42523/for-the-given-graph-show-how-clique-percolation--1/



