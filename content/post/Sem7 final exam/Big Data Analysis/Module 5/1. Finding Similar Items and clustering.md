---
title: Module 5 1  Finding Similar Items and clustering
date: 2025-01-10
---

### Syllabus
![alt text](Pastedimage20241108113821.png)

### Distance Measure:

In big data analysis, **distance measures** are used to ==calculate the similarity or dissimilarity between data points==, which is ==essential== in tasks like ==clustering, classification, anomaly detection, and recommendation systems==. The selection of a distance measure depends on the type of data (numerical, categorical, text, etc.) and the application. Here are some common distance measures and their use cases:

#### 1. **Euclidean Distance**

- **Formula**: $d(x,y)= \sqrt{\sum_{i=1}^n (x_i - y_i)^2}$
- **Use Case**: Commonly ==used in clustering algorithms== (e.g., ==K-means== ==clustering==) and nearest neighbor methods. It works well for numerical, continuous data.
- **Properties**: Measures the "straight-line" distance between two points in Euclidean space. It is sensitive to scale, so data normalization is often needed.

#### 2. **Cosine Similarity (Cosine Distance)**

- **Formula**: $similarity(x,y) = \frac{x \cdot y}{\|x\| \|y\|}​$
    - **Cosine Distance**: d(x,y) = 1 − cosine similarity
- **Use Case**: Widely used in text mining and document similarity, especially with sparse vectors (e.g., TF-IDF vectors in natural language processing).
- **Properties**: Measures the cosine of the angle between two vectors, making it useful when only the direction (not the magnitude) of vectors matters.

#### 3. **Jaccard Similarity (Jaccard Index)**

- **Formula**: $Jaccard(A,B) = \frac{|A \cap B|}{|A \cup B|}​$
    - **Jaccard Distance**: $d(A,B)=1−Jaccard(A,B)$
- **Use Case**: Commonly used in categorical data, binary attributes, and set-based data (e.g., user-item preferences in recommendation systems).
- **Properties**: Measures the similarity between two sets as the ratio of their intersection to their union.

#### 4. **Hamming Distance**

- **Formula**:
![alt text](Pastedimage20241108132504.png)
- **Use Case**: Used for categorical or binary data, especially for text comparison (e.g., in DNA sequencing, error detection/correction).
- **Properties**: Counts the number of differing elements between two strings of equal length. Suitable for comparing categorical data or binary feature vectors.

#### 5. **Edit Distance (Levenshtein Distance)**

![alt text](Pastedimage20241108133606.png)

- **Formula**: The minimum number of edits (insertions, deletions, substitutions) needed to transform one string into another.
- **Use Case**: Common in text and DNA sequence analysis, spell-checking, and natural language processing.
- **Properties**: Measures the dissimilarity between sequences; useful in cases where sequence order is important.

### CURE ALGORITHM (Clustering using algorithm)

![alt text](Pastedimage20241108133851.png)

![alt text](Pastedimage20241108133950.png)

The **CURE (==Clustering Using Representatives==)** algorithm is a ==hierarchical clustering algorithm== designed to ==handle large datasets== and ==address some limitations of traditional clustering methods, particularly with irregularly shaped clusters and large variations in cluster sizes==. Developed to be efficient for big data, CURE is especially useful in cases where other clustering algorithms (like K-means) might fail due to assumptions about cluster shapes or sizes.

#### Key Features of CURE

1. ==**Multiple Representative Points**==: CURE uses multiple points to represent each cluster, not just a centroid or a single average point. This approach makes it more resilient to different shapes and scales in data.
  
2. **Scalability**: By using random sampling and partitioning strategies, CURE can scale well with large datasets, a necessity in big data contexts.

3. ==**Hierarchical Approach**==: CURE is an agglomerative (bottom-up) hierarchical clustering algorithm. It initially treats each data point as an individual cluster and iteratively merges clusters based on the distance between their representative points.

4. **Handling Outliers**: CURE is ==effective in handling outliers and noise due to its use of representative points==. Outliers do not unduly influence cluster centroids or shapes.

#### CURE Algorithm Steps in Detail

Here's a step-by-step description of the CURE algorithm:

#### Step 1: Data Sampling
- Since working with the full dataset may be computationally intensive, CURE ==first randomly samples a subset of the dataset==.
- This sampling reduces computation while preserving the general structure of the clusters.

#### Step 2: Initial Clustering (Partitioning)
- The algorithm then ==partitions the sampled data into smaller subsets==, each containing a manageable number of points.
- Each ==subset is clustered individually using a fast hierarchical== clustering method (like ==agglomerative clustering==) until the subset size is reduced to a target number of clusters.
- This step allows CURE to quickly produce an initial set of clusters, making it feasible to process large datasets.

#### Step 3: Selecting Representative Points
- For each cluster created in the initial clustering phase, CURE selects a set of points called ==**representative points**== to capture the geometry and distribution of the data points in that cluster.
- The algorithm typically chooses these points by selecting a number of data points farthest away from the cluster's centroid. The exact number of representative points is a parameter of the algorithm.
- **Shrinkage**: Each representative point is then “shrunk” or moved closer to the cluster's centroid by a certain factor. This shrinkage factor is another parameter of the algorithm, which helps reduce the impact of outliers.

#### Step 4: Hierarchical Clustering on Representative Points
- CURE applies an ==agglomerative hierarchical clustering== method to merge clusters based on the distances between their representative points.
- In each iteration, CURE merges the two clusters that have the **closest pair of representative points** (measured by ==Euclidean distance==).
- This process continues until a desired number of clusters is achieved or the minimum distance between clusters exceeds a specified threshold.

#### Step 5: Assigning Non-Sampled Points to Clusters
- After the main clustering is complete, the remaining (non-sampled) data points are assigned to the nearest cluster based on the distance to the representative points of each cluster.
- This step ensures that the entire dataset is clustered, not just the initial sample.

#### Parameters of the CURE Algorithm

- **Number of Representative Points (k)**: The number of points chosen to represent each cluster. More representative points result in a better approximation of the cluster shape but increase computational complexity.
  
- **Shrinkage Factor $( \alpha )$**: A value between 0 and 1 that determines how much each representative point is moved toward the cluster centroid. A higher shrinkage factor reduces the influence of outliers.

- **Number of Clusters (c)**: The final number of clusters desired. This parameter can also be based on a threshold distance to control when the clustering process stops.

#### Advantages of CURE

1. **Flexibility with Cluster Shapes**: CURE handles irregularly shaped clusters better than centroid-based algorithms like K-means, thanks to its use of multiple representative points.
  
2. **Reduced Impact of Outliers**: The shrinkage factor helps mitigate the effect of outliers on cluster shapes.

3. **Scalability**: The use of random sampling and partitioning helps CURE scale efficiently for large datasets.

4. **Hierarchical Structure**: As a hierarchical method, CURE provides a structure that allows for easy interpretation and visualization of the data hierarchy, which is useful for exploring data patterns.

#### Limitations of CURE

1. **Parameter Sensitivity**: The performance and results of CURE are sensitive to the values of parameters like the number of representative points and shrinkage factor. Finding the optimal parameters can require experimentation.
  
2. **Computational Complexity**: Although it is optimized for large datasets, CURE still involves multiple clustering steps, which can be computationally intensive for extremely large datasets if sufficient resources aren’t available.

3. **Memory Requirements**: The algorithm needs to store multiple representative points for each cluster, which can consume significant memory when handling a large number of clusters or representative points.

#### Example of CURE Algorithm in Action

Imagine a dataset with two large clusters shaped like "donuts" and one small spherical cluster. Traditional methods might have difficulty distinguishing these shapes because of how they represent clusters with a single centroid.

Using CURE:

1. **Sampling and Partitioning**: CURE randomly samples the dataset and partitions it into manageable subsets.
  
2. **Initial Clustering**: Each subset is clustered hierarchically to generate an initial set of small clusters.

3. **Representative Points and Shrinkage**: For each initial cluster, CURE chooses a specified number of representative points spread around the cluster, then shrinks these points toward the centroid.

4. **Agglomerative Clustering**: CURE then performs hierarchical clustering on these representative points, gradually merging clusters based on the closest pair of representative points until the desired number of clusters is reached.

5. **Final Clustering**: The remaining data points (not part of the sample) are assigned to the nearest final clusters.

In this example, CURE’s use of multiple representative points allows it to capture the donut-shaped clusters’ boundary and interior points, handling the shape complexity that other algorithms might miss.

#### Applications of CURE

1. **Geospatial Data Analysis**: Identifying clusters in spatial data where shapes are irregular and varied, such as in geographical location clustering.
  
2. **Image Segmentation**: Separating clusters in image data, where clusters might not always have uniform shapes or sizes.

3. **Social Network Analysis**: Detecting communities or groups with non-uniform structures in social network data.

4. **Market Segmentation**: Grouping customer data where customer behavior patterns (clusters) vary significantly in shape and size.

In summary, CURE is a powerful clustering algorithm for large datasets with complex cluster shapes, providing flexibility and adaptability that simpler algorithms like K-means cannot offer. It achieves this through sampling, partitioning, and the innovative use of multiple representative points, making it suitable for diverse applications in big data analysis.